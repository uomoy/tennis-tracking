{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b989898a",
   "metadata": {},
   "source": [
    "# Tracking Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647bac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import py-motmetrics\n",
    "!pip install motmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b22230",
   "metadata": {},
   "source": [
    "  ## MOT Metrics Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58cdce5",
   "metadata": {},
   "source": [
    "* py-motmetrics 패키지를 활용하여 추적 성능을 측정 ([MOT16](https://arxiv.org/abs/1603.00831) Challenge 기반)\n",
    "* metrics 계산 함수는 [py-mottmotrics의 예시 함수](https://github.com/cheind/py-motmetrics?tab=readme-ov-file#for-custom-dataset) 코드를 기반으로 함\n",
    "* 해당 ipynb파일에서는 ByteTrack 수정 전후의 공에 대한 추적 성능을 비교하기 위해 ```only_ball=True``` argument를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "592f7a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a custom data evaluation function\n",
    "def motMetricsEnhancedCalculator(gtSource, tSource, only_ball=False):\n",
    "    \"\"\"\n",
    "    ground truth annotation과 tracking prediction annnotion을 통해 MOT metrics를 계산하고 결과를 출력함\n",
    "    \n",
    "        :gtSource (string): ground truth annotation (.txt) 파일 경로\n",
    "        :tSource (string): tracking prediction annotation (.txt) 파일 경로\n",
    "        :only_ball (bool): True라면 tennis_ball 클래스(1)에 대해서만 metrics 계산,\n",
    "                            False일 경우 선수(0)와 공(1) 모두에 대해 계산\n",
    "    \"\"\"\n",
    "    # import required packages\n",
    "    import motmetrics as mm\n",
    "    import numpy as np\n",
    "\n",
    "    # load ground truth\n",
    "    gt = np.loadtxt(gtSource, delimiter=',')\n",
    "\n",
    "    # load tracking output\n",
    "    t = np.loadtxt(tSource, delimiter=',')\n",
    "    \n",
    "    \n",
    "    # if you want to calculate metrics only with tennis_ball class(1)\n",
    "    if only_ball:\n",
    "        gt = gt[gt[:,7] == 1]\n",
    "        t = t[t[:,7] == 1]\n",
    "\n",
    "    # Create an accumulator that will be updated during each frame\n",
    "    acc = mm.MOTAccumulator(auto_id=True)\n",
    "\n",
    "    # Max frame number maybe different for gt and t files\n",
    "    for frame in range(int(gt[:,0].max())):\n",
    "        frame += 1 # detection and frame numbers begin at 1\n",
    "\n",
    "        # select id, x, y, width, height for current frame\n",
    "        # required format for distance calculation is X, Y, Width, Height \\\n",
    "        # We already have this format\n",
    "        gt_dets = gt[gt[:,0]==frame,1:6] # select all detections in gt\n",
    "        t_dets = t[t[:,0]==frame,1:6] # select all detections in t\n",
    "\n",
    "        C = mm.distances.iou_matrix(gt_dets[:,1:], t_dets[:,1:], \\\n",
    "                                    max_iou=0.5) # format: gt, t\n",
    "\n",
    "        # Call update once for per frame.\n",
    "        # format: gt object ids, t object ids, distance\n",
    "        acc.update(gt_dets[:,0].astype('int').tolist(), \\\n",
    "                  t_dets[:,0].astype('int').tolist(), C)\n",
    "\n",
    "    mh = mm.metrics.create()\n",
    "\n",
    "    summary = mh.compute(acc, metrics=['num_frames', 'idf1', 'idp', 'idr', \n",
    "                                     'recall', 'precision', 'num_objects', \n",
    "                                     'mostly_tracked', 'partially_tracked', \n",
    "                                     'mostly_lost', 'num_false_positives', \n",
    "                                     'num_misses', 'num_switches', \n",
    "                                     'num_fragmentations', 'mota', 'motp', \n",
    "                                       'num_detections'\n",
    "                                    ], \n",
    "                      name='acc')\n",
    "\n",
    "    strsummary = mm.io.render_summary(\n",
    "      summary,\n",
    "      #formatters={'mota' : '{:.2%}'.format},\n",
    "      namemap={'idf1': 'IDF1', 'idp': 'IDP', 'idr': 'IDR', 'recall': 'Rcll', \n",
    "               'precision': 'Prcn', 'num_objects': 'GT', \n",
    "               'mostly_tracked' : 'MT', 'partially_tracked': 'PT', \n",
    "               'mostly_lost' : 'ML', 'num_false_positives': 'FP', \n",
    "               'num_misses': 'FN', 'num_switches' : 'IDsw', \n",
    "               'num_fragmentations' : 'FM', 'mota': 'MOTA', 'motp' : 'MOTP', \n",
    "               'num_detections' : 'DET'\n",
    "              }\n",
    "    )\n",
    "    print(strsummary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e561a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOT metrics of <<HARD>> court :\n",
      "     num_frames      IDF1       IDP       IDR      Rcll      Prcn   GT  MT  PT  ML  FP  FN  IDsw  FM      MOTA      MOTP  DET\n",
      "acc         359  0.286149  0.294671  0.278107  0.908284  0.962382  338   1   0   0  12  31     3  12  0.863905  0.115443  307\n",
      "MOT metrics of <<GRASS>> court :\n",
      "     num_frames      IDF1       IDP       IDR      Rcll      Prcn   GT  MT  PT  ML  FP  FN  IDsw  FM      MOTA      MOTP  DET\n",
      "acc         349  0.417549  0.414414  0.420732  0.878049  0.864865  328   1   0   0  45  40     2  16  0.734756  0.129448  288\n",
      "MOT metrics of <<CLAY>> court :\n",
      "     num_frames      IDF1       IDP       IDR      Rcll      Prcn   GT  MT  PT  ML  FP  FN  IDsw  FM     MOTA      MOTP  DET\n",
      "acc         442  0.212766  0.218447  0.207373  0.854839  0.900485  434   1   0   0  41  63     7  21  0.74424  0.117773  371\n"
     ]
    }
   ],
   "source": [
    "# Calculate the MOT metrics for modified ByteTrack\n",
    "\n",
    "print('MOT metrics of <<HARD>> court :')\n",
    "motMetricsEnhancedCalculator('evaluation/test_annotation/TENNIS-HARD-GT.txt', \n",
    "                             'evaluation/test_annotation/TENNIS-HARD-PRED.txt',\n",
    "                              only_ball=True)\n",
    "\n",
    "print('MOT metrics of <<GRASS>> court :')\n",
    "motMetricsEnhancedCalculator('evaluation/test_annotation/TENNIS-GRASS-GT.txt', \n",
    "                             'evaluation/test_annotation/TENNIS-GRASS-PRED.txt',\n",
    "                              only_ball=True)\n",
    "\n",
    "print('MOT metrics of <<CLAY>> court :')\n",
    "motMetricsEnhancedCalculator('evaluation/test_annotation/TENNIS-CLAY-GT.txt', \n",
    "                             'evaluation/test_annotation/TENNIS-CLAY-PRED.txt',\n",
    "                              only_ball=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7f9abde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOT metrics of <<HARD>> court (default ByteTrack) :\n",
      "     num_frames      IDF1       IDP       IDR      Rcll      Prcn   GT  MT  PT  ML  FP   FN  IDsw  FM      MOTA      MOTP  DET\n",
      "acc         359  0.257391  0.312236  0.218935  0.683432  0.974684  338   0   1   0   6  107     8  12  0.642012  0.101981  231\n",
      "MOT metrics of <<GRASS>> court (default ByteTrack) :\n",
      "     num_frames      IDF1       IDP       IDR      Rcll     Prcn   GT  MT  PT  ML  FP  FN  IDsw  FM      MOTA      MOTP  DET\n",
      "acc         349  0.141509  0.146104  0.137195  0.807927  0.86039  328   1   0   0  43  63    15  22  0.631098  0.123922  265\n",
      "MOT metrics of <<CLAY>> court (default ByteTrack) :\n",
      "     num_frames      IDF1       IDP       IDR      Rcll      Prcn   GT  MT  PT  ML  FP   FN  IDsw  FM      MOTA      MOTP  DET\n",
      "acc         442  0.102696  0.115942  0.092166  0.711982  0.895652  434   0   1   0  36  125    22  27  0.578341  0.105572  309\n"
     ]
    }
   ],
   "source": [
    "# Calculate the MOT metrics for default ByteTrack (baseline)\n",
    "print('MOT metrics of <<HARD>> court (default ByteTrack) :')\n",
    "motMetricsEnhancedCalculator('evaluation/test_annotation/TENNIS-HARD-GT.txt', \n",
    "                             'evaluation/test_annotation/TENNIS-HARD-PRED-BYTETRACK.txt',\n",
    "                              only_ball=True)\n",
    "\n",
    "print('MOT metrics of <<GRASS>> court (default ByteTrack) :')\n",
    "motMetricsEnhancedCalculator('evaluation/test_annotation/TENNIS-GRASS-GT.txt', \n",
    "                             'evaluation/test_annotation/TENNIS-GRASS-PRED-BYTETRACK.txt',\n",
    "                              only_ball=True)\n",
    "\n",
    "print('MOT metrics of <<CLAY>> court (default ByteTrack) :')\n",
    "motMetricsEnhancedCalculator('evaluation/test_annotation/TENNIS-CLAY-GT.txt', \n",
    "                             'evaluation/test_annotation/TENNIS-CLAY-PRED-BYTETRACK.txt',\n",
    "                              only_ball=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72c50fe",
   "metadata": {},
   "source": [
    "## Inference Time Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e3655d",
   "metadata": {},
   "source": [
    "* 별도로 저장한 트래킹 로그(/evaluation/tracking_logs.py)에서 평균 추론시간을 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8179623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_processing_times(log_text):\n",
    "    \"\"\"\n",
    "    트래킹 로그로부터 평균 처리 시간(preprocess, inference, postprocess time)을 계산 \n",
    "    \n",
    "        :log_text (string): 한 영상 입력에 대한 YOLO_model.track메소드 실행 결과 텍스트\n",
    "        \n",
    "        :average_preprocess_time (float):  한 프레임당 평균 preprocess time (ms)\n",
    "        :average_inference_time (float): 한 프레임당 평균 inference time (ms)\n",
    "        :average_postprocess_time (float): 한 프레임당 평균 postprocess time (ms)\n",
    "    \"\"\"\n",
    "    import re\n",
    "    preprocess_times = []\n",
    "    inference_times = []\n",
    "    postprocess_times = []\n",
    "\n",
    "    matches = re.finditer(r'Speed: (\\d+\\.\\d+)ms preprocess, (\\d+\\.\\d+)ms inference, (\\d+\\.\\d+)ms postprocess', log_text)\n",
    "    for match in matches:\n",
    "        preprocess_time = float(match.group(1))\n",
    "        inference_time = float(match.group(2))\n",
    "        postprocess_time = float(match.group(3))\n",
    "\n",
    "        preprocess_times.append(preprocess_time)\n",
    "        inference_times.append(inference_time)\n",
    "        postprocess_times.append(postprocess_time)\n",
    "\n",
    "    average_preprocess_time = sum(preprocess_times) / len(preprocess_times) if preprocess_times else None\n",
    "    average_inference_time = sum(inference_times) / len(inference_times) if inference_times else None\n",
    "    average_postprocess_time = sum(postprocess_times) / len(postprocess_times) if postprocess_times else None\n",
    "\n",
    "    return average_preprocess_time, average_inference_time, average_postprocess_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99bf034a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<HARD>> court inference time: \n",
      "Average Preprocess Time: 1.546239554317549ms\n",
      "Average Inference Time: 8.81699164345404ms\n",
      "Average Postprocess Time: 1.2997214484679664ms\n",
      "Total Average Time: 11.662952646239555ms\n",
      "\n",
      "<<GRASS>> court inference time: \n",
      "Average Preprocess Time: 1.5650429799426935ms\n",
      "Average Inference Time: 8.959025787965617ms\n",
      "Average Postprocess Time: 1.3200573065902579ms\n",
      "Total Average Time: 11.844126074498568ms\n",
      "\n",
      "<<CLAY>> court inference time: \n",
      "Average Preprocess Time: 1.4076923076923078ms\n",
      "Average Inference Time: 7.866742081447963ms\n",
      "Average Postprocess Time: 1.1778280542986423ms\n",
      "Total Average Time: 10.452262443438913ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 함수 호출 및 출력\n",
    "from evaluation.tracking_logs import hard_logs_text, grass_logs_text, clay_logs_text # tracking_logs.py에서 각 로그(string) 불러오기\n",
    "\n",
    "logs = [hard_logs_text, grass_logs_text, clay_logs_text]\n",
    "courts = ['HARD', 'GRASS', 'CLAY']\n",
    "\n",
    "for court, log in zip(courts, logs):\n",
    "    average_preprocess, average_inference, average_postprocess = average_processing_times(log)\n",
    "    \n",
    "    print('<<'+court+'>> court inference time: ')\n",
    "    \n",
    "    if average_preprocess is not None:\n",
    "        print(f\"Average Preprocess Time: {average_preprocess}ms\")\n",
    "\n",
    "    if average_inference is not None:\n",
    "        print(f\"Average Inference Time: {average_inference}ms\")\n",
    "\n",
    "    if average_postprocess is not None:\n",
    "        print(f\"Average Postprocess Time: {average_postprocess}ms\")\n",
    "\n",
    "    if all([average_preprocess, average_inference, average_postprocess]):\n",
    "        total_average_time = average_preprocess + average_inference + average_postprocess\n",
    "        print(f\"Total Average Time: {total_average_time}ms\")\n",
    "    else:\n",
    "        print(\"Some processing time data not found in the logs.\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
